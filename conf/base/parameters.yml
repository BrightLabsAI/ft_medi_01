dataset:
  dataset_name: "ShenRuililin/MedicalQnA"
  dataset_percentage: 0.01
model:
  model_name: "distilbert/distilgpt2"

model_optimization:
  quantization:
    load_in_4bit: true
    bnb_4bit_quant_type: "nf4"
    bnb_4bit_use_double_quant: true
  lora:
    r: 8
    lora_alpha: 32
    lora_dropout: 0.1
    target_modules: ['q_proj', 'k_proj', 'v_proj', 'ffn', 'intermediate', 'layer_norm']

model_training:
  eval_steps: 5000
  learning_rate: 2e-5
  weight_decay: 0.01
  per_device_train_batch_size: 16
  gradient_accumulation_steps: 2
  fp16: true
